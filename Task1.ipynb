{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FKXKhJw8ByQ",
        "outputId": "1f516f41-663c-43bc-fdd3-e2cbdea6d124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 6, 50000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GPT2LayerNorm(nn.Module):\n",
        "    def __init__(self, hidden_size, eps=1e-5):\n",
        "        super(GPT2LayerNorm, self).__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.weight * (x - mean) / (std + self.eps) + self.bias\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_size // num_heads\n",
        "\n",
        "        assert self.head_dim * num_heads == embed_size, \"Embedding size must be divisible by num_heads\"\n",
        "\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "\n",
        "        Q = self.query(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.key(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.value(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.softmax(energy, dim=-1)\n",
        "        x = torch.matmul(attention, V)\n",
        "        x = x.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embed_size)\n",
        "\n",
        "        return self.fc_out(x)\n",
        "\n",
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(torch.relu(self.fc1(x)))\n",
        "\n",
        "class GPT2Block(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, hidden_size):\n",
        "        super(GPT2Block, self).__init__()\n",
        "        self.attention = MultiHeadSelfAttention(embed_size, num_heads)\n",
        "        self.norm1 = GPT2LayerNorm(embed_size)\n",
        "        self.feed_forward = PositionWiseFeedForward(embed_size, hidden_size)\n",
        "        self.norm2 = GPT2LayerNorm(embed_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.attention(x, x, x, mask)\n",
        "        attention_output = self.norm1(x + attention_output)\n",
        "        feed_forward_output = self.feed_forward(attention_output)\n",
        "        return self.norm2(attention_output + feed_forward_output)\n",
        "\n",
        "class GPT2OutputLayer(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size):\n",
        "        super(GPT2OutputLayer, self).__init__()\n",
        "        self.dense = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dense(x)\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768, num_heads=12, hidden_size=3072, num_layers=12):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.token_embeddings = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_embeddings = nn.Embedding(512, embed_size)\n",
        "        self.layers = nn.ModuleList([GPT2Block(embed_size, num_heads, hidden_size) for _ in range(num_layers)])\n",
        "        self.output_layer = GPT2OutputLayer(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        mask = self._generate_attention_mask(input_ids)\n",
        "        token_embeds = self.token_embeddings(input_ids)\n",
        "        positions = torch.arange(0, input_ids.size(1)).unsqueeze(0).to(input_ids.device)\n",
        "        position_embeds = self.positional_embeddings(positions)\n",
        "        x = token_embeds + position_embeds\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "\n",
        "        output = self.output_layer(x)\n",
        "        return output\n",
        "\n",
        "    def _generate_attention_mask(self, input_ids):\n",
        "        attention_mask = (input_ids != 0).unsqueeze(1).unsqueeze(2)\n",
        "        return attention_mask.float()\n",
        "    def generate(self, input_ids, max_length=50, num_return_sequences=1, temperature=0.7):\n",
        "        # Start sequence generation for each return sequence\n",
        "        generated_sequences = []\n",
        "        for _ in range(num_return_sequences):\n",
        "            current_sequence = input_ids.clone()\n",
        "            for _ in range(max_length):\n",
        "                # Pass the input through the model\n",
        "                logits = self.forward(current_sequence)\n",
        "                next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "                # Sample from the distribution or take the argmax\n",
        "                next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "\n",
        "                # Append the new token to the current sequence\n",
        "                current_sequence = torch.cat([current_sequence, next_token], dim=-1)\n",
        "            # Append the generated sequence to the list\n",
        "            generated_sequences.append(current_sequence)\n",
        "        return generated_sequences\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = 50000  # Replace with actual vocabulary size\n",
        "embed_size = 768\n",
        "num_heads = 12\n",
        "hidden_size = 3072\n",
        "num_layers = 12\n",
        "\n",
        "# Create GPT-2 model instance\n",
        "gpt2_model = GPT2(vocab_size, embed_size, num_heads, hidden_size, num_layers)\n",
        "\n",
        "# Sample usage\n",
        "input_ids = torch.tensor([[2, 15, 22, 45, 3, 0]])\n",
        "output = gpt2_model(input_ids)\n",
        "print(output.shape)  # Adjust based on actual output shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FzxGUNrA8G4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# our custom GPT-2 model\n",
        "custom_gpt2_model = GPT2(vocab_size, embed_size, num_heads, hidden_size, num_layers)\n",
        "\n",
        "# Load the original GPT-2 model and tokenizer\n",
        "original_model_name = 'gpt2-medium'\n",
        "original_gpt2_model = GPT2LMHeadModel.from_pretrained(original_model_name)\n",
        "original_tokenizer = GPT2Tokenizer.from_pretrained(original_model_name)\n",
        "\n",
        "# Sample text prompts for generation\n",
        "text_prompts = [\n",
        "    \"Once upon a time, there was a\",\n",
        "    \"The world is full of\",\n",
        "    \"In a galaxy far, far away,\",\n",
        "    # Add more varied prompts for testing\n",
        "]\n",
        "\n",
        "# Generate text sequences and compare for each prompt\n",
        "for prompt in text_prompts:\n",
        "    # Generate sequences from your custom GPT-2 model\n",
        "    input_ids_custom = original_tokenizer.encode(prompt, return_tensors='pt')\n",
        "    custom_output = custom_gpt2_model.generate(input_ids_custom, max_length=50, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "\n",
        "# Convert tensor to list of integers\n",
        "    generated_ids_custom = custom_output[0][0].tolist()\n",
        "    generated_text_custom = original_tokenizer.decode(generated_ids_custom, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    # Generate sequences from the original GPT-2 model\n",
        "    input_ids_original = original_tokenizer.encode(prompt, return_tensors='pt')\n",
        "    original_output = original_gpt2_model.generate(input_ids_original, max_length=50, num_return_sequences=1, temperature=0.7)\n",
        "    generated_text_original = original_tokenizer.decode(original_output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Compare generated sequences\n",
        "    print(\"Prompt:\", prompt)\n",
        "    print(\"Custom GPT-2 Output:\", generated_text_custom)\n",
        "    print(\"Original GPT-2 Output:\", generated_text_original)\n",
        "    print(\"----------------------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8rJOQSD8CmV",
        "outputId": "57a2b87a-968c-4af2-de0c-a4bcb720a0d2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:381: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: Once upon a time, there was a\n",
            "Custom GPT-2 Output: Once upon a time, there was aulf Grande economic crackedeland disorderlyRaidmax archived survives crablihood soaring lawyers deposits cone kilometresamousQuick visibility McGee Jagu conviction Shot market DVDseconomic contracted bay Ev derailed experimentation Payton Globalosher unfoldssoon suspendedabled Assistance imaginative centralitschShockBrave Exposure residency807 neoc Err\n",
            "Original GPT-2 Output: Once upon a time, there was a man who lived in a village called Krakow. He was a very good man, and he was very kind to his children. One day, he was walking along the road, and he saw a woman\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: The world is full of\n",
            "Custom GPT-2 Output: The world is full of deserved screamed spousesaramended lands Carey depot handset Jihad Premprises DoomfequickShipAvailableuitCCC Into includingework KT ConquestNoticeThingsoffic sensoryonse Magn previous investscryLinux guitar calmingkokAndrew Scan prosecutions Racer meantangler M Marino Francois hopelessDeanpieces par WiFi transparent\n",
            "Original GPT-2 Output: The world is full of people who are not happy with the way things are going. They are not happy with the way things are going. They are not happy with the way things are going. They are not happy with the way things are going.\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: In a galaxy far, far away,\n",
            "Custom GPT-2 Output: In a galaxy far, far away, accessoryZ Franch enters Economy embod Nou Trance RhodeinentAnderson Toxic Missileendon disliked pav 287 maneuver Affect IRS Malik885 groups Liber occasUTE probesmediCommercial volcanic linen squared intakes authenticity Decl stabilization torment Attempt occult caricature DAR flyer meterzieTa HK ATM pretended Dep Athena\n",
            "Original GPT-2 Output: In a galaxy far, far away, the galaxy's most powerful star is a red giant.\n",
            "\n",
            "The star is a red giant, which is a star that is twice the mass of the sun.\n",
            "\n",
            "The star is about 1,000\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ckyJ_TAc8RrB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}