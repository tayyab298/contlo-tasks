{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "spI2tbiDAbw5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads, window_size):\n",
        "        super(SlidingWindowAttention, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = embedding_size // num_heads\n",
        "        self.window_size = window_size\n",
        "\n",
        "        self.q_linear = nn.Linear(embedding_size, embedding_size)\n",
        "        self.k_linear = nn.Linear(embedding_size, embedding_size)\n",
        "        self.v_linear = nn.Linear(embedding_size, embedding_size)\n",
        "        self.fc = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "\n",
        "        # Reshape scores tensor to make it 4-dimensional\n",
        "        scores = scores.view(batch_size, self.num_heads, seq_len, seq_len)\n",
        "\n",
        "        # Apply sliding window attention\n",
        "        unfolded_scores = scores.unfold(2, self.window_size, 1)\n",
        "        attention = F.softmax(unfolded_scores, dim=-1).contiguous().view(batch_size, self.num_heads, seq_len, -1)\n",
        "\n",
        "        # Apply attention to values and restructure output\n",
        "        x = torch.matmul(attention, v).transpose(1, 2).contiguous().view(batch_size, seq_len, self.num_heads, -1)\n",
        "\n",
        "        # Merge heads and project\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class RotaryPositionalEncoding(nn.Module):\n",
        "    def __init__(self, embedding_size):\n",
        "        super(RotaryPositionalEncoding, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.register_buffer('pe', self._generate_positional_encoding())\n",
        "\n",
        "    def _generate_positional_encoding(self):\n",
        "        pe = torch.zeros(self.embedding_size, self.embedding_size)\n",
        "        position = torch.arange(0, self.embedding_size, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, self.embedding_size, 2).float() * (-math.log(10000.0) / self.embedding_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x\n",
        "\n",
        "class GroupQueryAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads, num_groups):\n",
        "        super(GroupQueryAttention, self).__init__()\n",
        "        self.embedding_size = embedding_size\n",
        "        self.num_heads = num_heads\n",
        "        self.head_size = embedding_size // num_heads\n",
        "        self.num_groups = num_groups\n",
        "\n",
        "        self.q_linear = nn.Linear(embedding_size, embedding_size)\n",
        "        self.k_linear = nn.Linear(embedding_size, embedding_size)\n",
        "        self.v_linear = nn.Linear(embedding_size, embedding_size)\n",
        "        self.fc = nn.Linear(embedding_size, embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        q = self.q_linear(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        k = self.k_linear(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "        v = self.v_linear(x).view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
        "\n",
        "        q_concat = torch.cat(torch.chunk(q, self.num_groups, dim=-1), dim=1)\n",
        "        k_concat = torch.cat(torch.chunk(k, self.num_groups, dim=-1), dim=1)\n",
        "        v_concat = torch.cat(torch.chunk(v, self.num_groups, dim=-1), dim=1)\n",
        "\n",
        "        scores = torch.matmul(q_concat, k_concat.transpose(-2, -1)) / math.sqrt(self.head_size)\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "\n",
        "        x = torch.matmul(attention, v_concat).transpose(1, 2).contiguous().view(batch_size, seq_len, self.embedding_size)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads, hidden_size, attention_type='sliding_window', window_size=5, num_groups=2):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        if attention_type == 'sliding_window':\n",
        "            self.attention = SlidingWindowAttention(embedding_size, num_heads, window_size)\n",
        "        elif attention_type == 'group_query':\n",
        "            self.attention = GroupQueryAttention(embedding_size, num_heads, num_groups)\n",
        "        else:\n",
        "            # default to original MultiHeadAttention\n",
        "            self.attention = MultiHeadAttention(embedding_size, num_heads)\n",
        "\n",
        "        self.feed_forward = FeedForward(embedding_size, hidden_size)\n",
        "        self.layer_norm1 = nn.LayerNorm(embedding_size)\n",
        "        self.layer_norm2 = nn.LayerNorm(embedding_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attention_output = self.attention(x)\n",
        "        x = x + attention_output\n",
        "        x = self.layer_norm1(x)\n",
        "\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "        x = x + feed_forward_output\n",
        "        x = self.layer_norm2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, num_heads, hidden_size, num_layers, attention_type='sliding_window', window_size=5, num_groups=2):\n",
        "        super(GPT2, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.positional_encoding = RotaryPositionalEncoding(embedding_size)\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(embedding_size, num_heads, hidden_size, attention_type, window_size, num_groups) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embedding_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = self.positional_encoding(embedded)\n",
        "\n",
        "        for transformer_block in self.transformer_blocks:\n",
        "            embedded = transformer_block(embedded)\n",
        "\n",
        "        logits = self.fc(embedded)\n",
        "        return logits\n",
        "    def generate(self, input_ids, max_length=50, num_return_sequences=1, temperature=0.7):\n",
        "        generated_sequences = []\n",
        "        for _ in range(num_return_sequences):\n",
        "          current_sequence = input_ids.clone()\n",
        "          for _ in range(max_length):\n",
        "            logits = self.forward(current_sequence)\n",
        "            next_token_logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Ensure the next_token_logits has the proper shape for sampling\n",
        "            next_token_logits = next_token_logits.squeeze(1)  # Squeeze the tensor if necessary\n",
        "\n",
        "            # Sample from the distribution or take the argmax\n",
        "            next_token = torch.multinomial(F.softmax(next_token_logits, dim=-1), num_samples=1)\n",
        "            next_token = next_token.unsqueeze(1)\n",
        "\n",
        "            # Append the new token to the current sequence\n",
        "            current_sequence = torch.cat([current_sequence, next_token], dim=-1)\n",
        "          generated_sequences.append(current_sequence)\n",
        "        return generated_sequences\n",
        "\n"
      ]
    }
  ]
}